"""
Provides utilities for saving and managing SGLD samples.

This module contains the `SamplerSaver` and `BayesianManager` classes, which
are used to save and manage the samples generated by the SGLD optimizer. It
also includes a function for loading SGLD models from saved checkpoints.
"""

import os
import torch
import glob
from src.common.base_saver import Saver, Manager
from typing import List


class SamplerSaver(Saver):
    """
    A class for saving and managing SGLD samples.

    This class is used to save the parameters of a model at different points
    during training, which is useful for Bayesian neural networks trained with
    methods like SGLD.

    Args:
        save_dir (str, optional): The directory to save the samples in.
            Defaults to "./".
        filename_prefix (str, optional): A prefix to add to the sample
            filenames. Defaults to None.
        batch_size (int, optional): The number of samples to collect before
            saving a batch to disk. Defaults to 1.
    """

    def __init__(
        self,
        save_dir: str = "./",
        filename_prefix: str = None,
        batch_size: int = 1,
    ):
        super(SamplerSaver, self).__init__(
            save_dir=save_dir, filename_prefix=filename_prefix
        )
        self.batch_size = batch_size
        self.sample_buffer = []
        self.batch_count = 0
        self.sample_count = 0

    def add_sample(self, model: torch.nn.Module):
        """
        Adds the current model's parameters to the sample buffer.

        Args:
            model (torch.nn.Module): The model to save the parameters of.
        """
        state_dict = model.state_dict()
        self.sample_buffer.append(state_dict)
        self.sample_count += 1

        if len(self.sample_buffer) >= self.batch_size:
            self.save_ensemble()
            self.sample_buffer = []

    def save_ensemble(self):
        """Saves the current batch of samples to disk."""
        if self.filename_prefix is None:
            filename = (
                f"sample_batch_{self.batch_count}_{len(self.sample_buffer)}"
                + ".pt"
            )
        else:
            filename = (
                self.filename_prefix
                + f"_batch_{self.batch_count}_{len(self.sample_buffer)}"
                + ".pt"
            )

        filepath = os.path.join(self.save_dir, filename)
        torch.save(self.sample_buffer, filepath)
        self.batch_count += 1

    def finalize(self):
        """Saves any remaining samples in the buffer."""
        if len(self.sample_buffer) > 0:
            self.save_ensemble()
            self.sample_buffer = []

    @staticmethod
    def load_samples(folder_path: str, file_pattern: str = "*.pt") -> List[dict]:
        """
        Loads all sample files from a folder.

        Args:
            folder_path (str): The path to the folder containing the sample
                files.
            file_pattern (str, optional): The pattern to match the sample
                files. Defaults to "*.pt".

        Returns:
            list: A list of all the loaded samples.
        """
        file_paths = glob.glob(os.path.join(folder_path, file_pattern))
        if not file_paths:
            print(
                f"No files matching pattern '{file_pattern}' found in {folder_path}"
            )
            return []

        all_samples = []
        for file_path in file_paths:
            try:
                loaded_data = torch.load(file_path)
                if not isinstance(loaded_data, list):
                    print(
                        f"Warning: File {file_path} does not contain a list. Skipping."
                    )
                    continue
                for i, item in enumerate(loaded_data):
                    if not isinstance(item, dict):
                        print(
                            f"Warning: Item {i} in file {file_path} is not a dictionary. Skipping this item."
                        )
                        continue
                    all_samples.append(item)
            except Exception as e:
                print(f"Error loading file {file_path}: {str(e)}")
        return all_samples

    def __len__(self) -> int:
        """
        Returns the total number of samples collected.

        Returns:
            int: The total number of samples collected.
        """
        return self.sample_count


class BayesianManager(Manager):
    """
    A class for managing the training of Bayesian neural networks.

    This class extends the base `Manager` class to handle the saving and
    evaluation schedule for Bayesian neural networks, and keeps track of the
    samples collected during training.

    Args:
        num_epochs (int, optional): The total number of training epochs.
            Defaults to 500.
        num_burnin_epochs (int, optional): The number of epochs to burn in
            before starting to collect samples. Defaults to 100.
        eval_freq (int, optional): The frequency (in epochs) at which to
            evaluate the model. Defaults to 1.
        ensemble_freq (int, optional): The frequency (in epochs) at which to
            collect samples. Defaults to 100.
        save_freq (int, optional): The frequency (in epochs) at which to
            save model checkpoints. Defaults to 10.
        save_dir (str, optional): The directory to save checkpoints and
            samples in. Defaults to "./".
        filename_prefix (str, optional): A prefix to add to the checkpoint
            and sample filenames. Defaults to None.
        batch_size (int, optional): The number of samples to collect before
            saving a batch to disk. Defaults to 1.
    """

    def __init__(
        self,
        num_epochs: int = 500,
        num_burnin_epochs: int = 100,
        eval_freq: int = 1,
        ensemble_freq: int = 100,
        save_freq: int = 10,
        save_dir: str = "./",
        filename_prefix: str = None,
        batch_size: int = 1,
    ):
        super(BayesianManager, self).__init__(
            num_epochs=num_epochs,
            eval_freq=eval_freq,
            save_freq=save_freq,
            save_dir=save_dir,
            filename_prefix=filename_prefix,
        )
        self.num_burnin_epochs = num_burnin_epochs
        self.ensemble_freq = ensemble_freq

        ensemble_dir = os.path.join(self.save_dir, "samples")
        os.makedirs(ensemble_dir, exist_ok=True)

        self.saver = SamplerSaver(
            save_dir=ensemble_dir,
            filename_prefix=filename_prefix,
            batch_size=batch_size,
        )

    def should_sample(self, epoch: int) -> bool:
        """
        Determines whether to collect samples at the given epoch.

        Args:
            epoch (int): The current epoch number (zero-based).

        Returns:
            bool: True if samples should be collected, False otherwise.
        """
        return ((epoch + 1) >= self.num_burnin_epochs) and (
            ((epoch + 1) - self.num_burnin_epochs) % self.ensemble_freq == 0
        )

    def save_sample(self, epoch: int, model: torch.nn.Module):
        """
        Saves a sample of the model's parameters if the conditions are met.

        Args:
            epoch (int): The current epoch number (zero-based).
            model (torch.nn.Module): The model to save the parameters of.
        """
        if self.should_sample(epoch):
            self.saver.add_sample(model)

    def finalize_sampling(self):
        """Finalizes the sampling process by saving any remaining samples."""
        return self.saver.finalize()

    def load_samples(self, filepath: str, file_pattern: str = "*.pt") -> List[dict]:
        """
        Loads all sample files from a folder.

        Args:
            filepath (str): The path to the folder containing the sample
                files.
            file_pattern (str, optional): The pattern to match the sample
                files. Defaults to "*.pt".

        Returns:
            list: A list of all the loaded samples.
        """
        return self.saver.load_samples(filepath, file_pattern)

    def __len__(self) -> int:
        """
        Returns the total number of samples collected.

        Returns:
            int: The total number of samples collected.
        """
        return len(self.saver)


def load_sgld_models(filepath: str, model_list: List[torch.nn.Module]) -> List[torch.nn.Module]:
    """
    Loads the state dicts of the best models in an SGLD ensemble from a
    directory.

    Args:
        filepath (str): The path to the directory containing the saved models.
        model_list (list of torch.nn.Module): A list of the models to load the
            state dicts into.

    Returns:
        list of torch.nn.Module: The list of models with the loaded state
            dicts.
    """
    num_models = len(model_list)

    for model_idx in range(num_models):
        path = os.path.join(filepath, f"model_{model_idx}_best.pt")
        checkpoint = torch.load(path, weights_only=False)
        model_list[model_idx].load_state_dict(checkpoint["model_state_dict"])

    return model_list